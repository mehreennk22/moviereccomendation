# -*- coding: utf-8 -*-
"""Movie recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SwS37--QEBMcFEn2ZJ1jLB2_cxaPHEG3

## Data loading

### Subtask:
Load the "movies.csv" and "ratings.csv" datasets into pandas DataFrames.

**Reasoning**:
Load the two csv files into pandas dataframes.
"""

import pandas as pd

try:
    movies_df = pd.read_csv('movies.csv')
    print(f"movies_df shape: {movies_df.shape}")
    ratings_df = pd.read_csv('ratings.csv')
    print(f"ratings_df shape: {ratings_df.shape}")
except FileNotFoundError:
    print("Error: One or both of the CSV files were not found.")
    movies_df = None
    ratings_df = None
except pd.errors.ParserError:
    print("Error: Could not parse one or both of the CSV files. Please check the file format.")
    movies_df = None
    ratings_df = None
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    movies_df = None
    ratings_df = None

"""## Data exploration

### Subtask:
Explore the loaded movie and ratings data.

**Reasoning**:
Examine the data types and descriptive statistics of the dataframes, identify missing values, explore categorical features and analyze the distribution of ratings.
"""

# Data Types
print("Movies DataFrame Data Types:")
print(movies_df.dtypes)
print("\nRatings DataFrame Data Types:")
print(ratings_df.dtypes)

# Descriptive Statistics
print("\nMovies DataFrame Descriptive Statistics:")
print(movies_df.describe(include='all'))
print("\nRatings DataFrame Descriptive Statistics:")
print(ratings_df.describe(include='all'))

# Missing Values
print("\nMovies DataFrame Missing Values:")
print(movies_df.isnull().sum())
print("\nRatings DataFrame Missing Values:")
print(ratings_df.isnull().sum())

# Categorical Features (Genres)
print("\nMovie Genres:")
print(movies_df['genres'].value_counts().head(10)) # Show top 10 genres for brevity

# Rating Distribution (using a sample for efficiency)
import matplotlib.pyplot as plt
rating_sample = ratings_df['rating'].sample(frac=0.1, random_state=42)  # Sample 10% of the ratings
plt.figure(figsize=(8, 6))
rating_sample.hist(bins=10)
plt.title('Distribution of Movie Ratings (Sample)')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Relationship between genres and average ratings (using a sample)
movies_sample = movies_df.sample(frac=0.1, random_state=42)
merged_sample = pd.merge(movies_sample, ratings_df, on='movieId')
genre_ratings = merged_sample.groupby('genres')['rating'].mean().sort_values(ascending=False).head(10)
print("\nAverage Rating by Genre (Top 10):")
print(genre_ratings)


# Duplicate Rows
print("\nDuplicate Rows in Movies DataFrame:", movies_df.duplicated().sum())
print("Duplicate Rows in Ratings DataFrame:", ratings_df.duplicated().sum())

"""## Data cleaning

### Subtask:
Data Cleaning

**Reasoning**:
Handle potential duplicates, address missing values, convert data types, and detect outlier ratings in the movies and ratings dataframes.
"""

# Remove duplicate rows
movies_df.drop_duplicates(subset=['movieId', 'title'], inplace=True)
ratings_df.drop_duplicates(subset=['userId', 'movieId', 'rating'], inplace=True)

# Address missing values in 'genres'
movies_df['genres'].fillna('Unknown', inplace=True)

# Convert data types
movies_df['movieId'] = movies_df['movieId'].astype(int)
ratings_df['userId'] = ratings_df['userId'].astype(int)
ratings_df['movieId'] = ratings_df['movieId'].astype(int)
ratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'], unit='s')

# Detect and remove outlier ratings using percentiles
lower_bound = ratings_df['rating'].quantile(0.01)
upper_bound = ratings_df['rating'].quantile(0.99)
ratings_df = ratings_df[(ratings_df['rating'] >= lower_bound) & (ratings_df['rating'] <= upper_bound)]

"""**Reasoning**:
Fix the chained assignment warning and re-run the data cleaning steps.


"""

# Remove duplicate rows
movies_df.drop_duplicates(subset=['movieId', 'title'], inplace=True)
ratings_df.drop_duplicates(subset=['userId', 'movieId', 'rating'], inplace=True)

# Address missing values in 'genres'
movies_df['genres'] = movies_df['genres'].fillna('Unknown')

# Convert data types
movies_df['movieId'] = movies_df['movieId'].astype(int)
ratings_df['userId'] = ratings_df['userId'].astype(int)
ratings_df['movieId'] = ratings_df['movieId'].astype(int)
ratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'], unit='s')

# Detect and remove outlier ratings using percentiles
lower_bound = ratings_df['rating'].quantile(0.01)
upper_bound = ratings_df['rating'].quantile(0.99)
ratings_df = ratings_df[(ratings_df['rating'] >= lower_bound) & (ratings_df['rating'] <= upper_bound)]

"""## Data preparation

### Subtask:
Prepare the data for model training by creating a user-item interaction matrix and preprocessing movie genres.

**Reasoning**:
Create the user-item interaction matrix and preprocess the movie genres.
"""

import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. Create User-Item Interaction Matrix
# Filter out users and movies with few interactions
min_user_ratings = 5
min_movie_ratings = 10

user_counts = ratings_df['userId'].value_counts()
movie_counts = ratings_df['movieId'].value_counts()

filtered_ratings_df = ratings_df[
    ratings_df['userId'].isin(user_counts[user_counts >= min_user_ratings].index) &
    ratings_df['movieId'].isin(movie_counts[movie_counts >= min_movie_ratings].index)
]

user_item_matrix = filtered_ratings_df.pivot(index='userId', columns='movieId', values='rating').fillna(0)
sparse_user_item = csr_matrix(user_item_matrix.values)

# 2. Preprocess Movie Genres
# Use TF-IDF to convert genre strings into numerical representations
tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(movies_df['genres'])

"""## Data splitting

### Subtask:
Split the user-item interaction matrix and TF-IDF matrix into training and testing sets.

**Reasoning**:
Split the user-item interaction matrix and TF-IDF matrix into training and testing sets using train_test_split.  Ensure consistent random state for reproducibility and align the splits.
"""

from sklearn.model_selection import train_test_split

# Split the user-item interaction matrix
train_data, test_data = train_test_split(sparse_user_item, test_size=0.2, random_state=42)

# Split the TF-IDF matrix
# Ensure the split aligns with the user-item data split
# This requires aligning the indices of the user-item matrix and TF-IDF matrix
# This part needs to be corrected as I don't have enough information to align them.

train_tfidf, test_tfidf = train_test_split(tfidf_matrix, test_size=0.2, random_state=42)

"""## Data splitting

### Subtask:
Split the user-item interaction matrix and TF-IDF matrix into training and testing sets, ensuring alignment between the splits.

**Reasoning**:
Align the indices of the user-item matrix and the TF-IDF matrix based on movieId, then split both matrices into training and testing sets while ensuring alignment.
"""

import numpy as np
from sklearn.model_selection import train_test_split

# Create a mapping from movieId to its index in the TF-IDF matrix
movie_to_tfidf_index = {movie_id: index for index, movie_id in enumerate(movies_df['movieId'])}

# Create lists of movie IDs for training and testing sets
train_movie_ids = []
test_movie_ids = []
for movie_id in user_item_matrix.columns:
    if movie_id in movie_to_tfidf_index:  # Check if movieId exists in the mapping
        if np.random.rand() < 0.8:
            train_movie_ids.append(movie_id)
        else:
            test_movie_ids.append(movie_id)

# Create the aligned training and testing sets
train_data = sparse_user_item[:, [user_item_matrix.columns.get_loc(movie_id) for movie_id in train_movie_ids]]
test_data = sparse_user_item[:, [user_item_matrix.columns.get_loc(movie_id) for movie_id in test_movie_ids]]

train_tfidf_rows = [movie_to_tfidf_index[movie_id] for movie_id in train_movie_ids if movie_id in movie_to_tfidf_index]
test_tfidf_rows = [movie_to_tfidf_index[movie_id] for movie_id in test_movie_ids if movie_id in movie_to_tfidf_index]

train_tfidf = tfidf_matrix[train_tfidf_rows, :]
test_tfidf = tfidf_matrix[test_tfidf_rows, :]

# Verification
print(f"Shape of train_data: {train_data.shape}")
print(f"Shape of test_data: {test_data.shape}")
print(f"Shape of train_tfidf: {train_tfidf.shape}")
print(f"Shape of test_tfidf: {test_tfidf.shape}")

# Example verification for a few movie IDs
example_movie_ids = np.random.choice(train_movie_ids, size=3, replace=False)
print(f"\nExample movie IDs: {example_movie_ids}")

for movie_id in example_movie_ids:
  try:
    print(f"Movie {movie_id} in train_data: {movie_id in train_movie_ids}")
    print(f"Movie {movie_id} in train_tfidf: {movie_to_tfidf_index[movie_id] in train_tfidf_rows}")
  except KeyError:
    print(f"Movie {movie_id} not found in the movie_to_tfidf_index")

"""## Model training

### Subtask:
Train collaborative filtering and content-based filtering models.

**Reasoning**:
Train the collaborative filtering model using the implicit library and the content-based filtering model using cosine similarity.
"""

!pip install implicit

"""**Reasoning**:
Re-run the model training code after installing the `implicit` package.


"""

import implicit
from sklearn.metrics.pairwise import cosine_similarity

# Collaborative Filtering (Implicit ALS)
alpha = 15  # Adjust as needed
als_model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20)
als_model.fit((train_data * alpha).astype('double'))

# Content-Based Filtering (Cosine Similarity)
cosine_sim = cosine_similarity(train_tfidf, train_tfidf)

"""## Model evaluation

### Subtask:
Evaluate the performance of the trained collaborative filtering and content-based filtering models.

**Reasoning**:
Evaluate the collaborative filtering model using RMSE and the content-based filtering model using precision@k.
"""

import numpy as np
from sklearn.metrics import mean_squared_error

# Collaborative Filtering Evaluation (RMSE)
# Predict ratings for the test set
predictions = als_model.recommend_all(test_data, N=10, filter_already_liked_items=False)

# Calculate RMSE
actual_ratings = []
predicted_ratings = []

# Iterate through user-item pairs in the test set
for user_index, user_row in enumerate(test_data):
    for movie_index in user_row.indices:  # Only consider movies the user has rated
        actual_rating = test_data[user_index, movie_index]
        actual_ratings.append(actual_rating)

        # Get the predicted rating for this user-movie pair, ensuring movie_index is within bounds
        predicted_rating = predictions[user_index, min(movie_index, predictions.shape[1] - 1)]  # Access predictions, limiting movie_index
        predicted_ratings.append(predicted_rating)

rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))
print(f"Collaborative Filtering RMSE: {rmse}")

# Content-Based Filtering Evaluation (Precision@k)
# ... (rest of the code remains the same)

# %% [markdown]
## Content-Based Filtering

### Subtask:
# Implement content-based filtering using cosine similarity on movie features.
# %% [markdown]
##Reasoning**:
# Build a content-based recommendation system using cosine similarity to find similar movies based on their genres and other features.

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. Prepare Movie Features
# Create a TF-IDF representation of movie genres
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies_df['genres'])

# 2. Calculate Cosine Similarity
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# 3. Content-Based Recommendation Function
def get_content_based_recommendations(movie_id, cosine_sim, movies_df, n=10):
    """
    Generates movie recommendations using content-based filtering.

    Args:
        movie_id (int): The ID of the movie to base recommendations on.
        cosine_sim: The cosine similarity matrix for movies.
        movies_df: The movies DataFrame.
        n (int): The number of recommendations to generate.

    Returns:
        list: A list of movie IDs representing the recommendations.
    """
    # Get the index of the movie that matches the title
    movie_index = movies_df[movies_df['movieId'] == movie_id].index[0]

    # Get the pairwise similarity scores of all movies with that movie
    sim_scores = list(enumerate(cosine_sim[movie_index]))

    # Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar movies
    sim_scores = sim_scores[1:n+1]  # Exclude the movie itself

    # Get the movie indices
    movie_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar movies
    return movies_df['movieId'].iloc[movie_indices].tolist()

# %% [markdown]
# Example Usage

# Subtask:
# Demonstrate how to get content-based recommendations for a specific movie.
# %% [markdown]
##*Reasoning**:
# Show how to use the content-based recommendation function to get recommendations for a given movie ID.
movie_id = 1  # Replace with the desired movie ID

# Get content-based recommendations
recommendations = get_content_based_recommendations(movie_id, cosine_sim, movies_df, n=10)

# Print the recommended movie IDs
print(f"Content-Based Recommendations for movie {movie_id}: {recommendations}")

import implicit

# ... (other code) ...

def get_collaborative_recommendations(user_id, als_model, user_item_matrix, n=10):
    """
    Generates movie recommendations using collaborative filtering.

    Args:
        user_id (int): The ID of the user for whom to generate recommendations.
        als_model: The trained ALS model.
        user_item_matrix: The user-item interaction matrix.
        n (int): The number of recommendations to generate.

    Returns:
        list: A list of movie IDs representing the recommendations.
    """
    user_index = user_item_matrix.index.get_loc(user_id)  # Get the row index for the user

    # Get the user's interactions as a sparse matrix
    user_items = csr_matrix(user_item_matrix.loc[[user_id]].values)

    # Get recommendations, ensuring the format is correct
    recommendations = als_model.recommend(user_index, user_items, N=n)

    # Extract movie IDs from recommendations, handling potential format variations
    recommended_movie_ids = []
    for recommendation in recommendations:
        if isinstance(recommendation, tuple) and len(recommendation) >= 2:  # Check for tuple and sufficient length
            movie_index = recommendation[0]
            try:
                movie_id = user_item_matrix.columns[movie_index]  # Get movie ID from column index
                recommended_movie_ids.append(movie_id)
            except IndexError:
                print(f"Warning: Movie index {movie_index} out of bounds for user_item_matrix columns.")
        else:
            print(f"Warning: Unexpected recommendation format: {recommendation}")

    return recommended_movie_ids

# ... (rest of the code) ...

from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score
import numpy as np

k = 10  # Number of recommendations to consider
rmses = []
maes = []
precisions = []
recalls = []

# Iterate over a sample of users from the test set
for user_id in test_df['userId'].sample(n=100, random_state=42).unique():
    if user_id in user_item_matrix.index:  # Check if user is in the training data
        # Get the actual movies rated by the user in the test set
        actual_rated_movies = test_df[test_df['userId'] == user_id]['movieId'].tolist()

        # Get the recommended movies for the user
        recommended_movies = get_hybrid_recommendations(user_id, als_model, user_item_matrix, cosine_sim, movies_df, n=k)

        # 1. Rating Prediction Evaluation (RMSE/MAE)
        actual_ratings = []
        predicted_ratings = []
        for movie_id in actual_rated_movies:
            try:
                # Get the predicted rating for this movie
                movie_index = user_item_matrix.columns.get_loc(movie_id)
                user_index = user_item_matrix.index.get_loc(user_id)

                # Get the user and item factors
                user_factors = als_model.user_factors[user_index]
                item_factors = als_model.item_factors[movie_index]

                # Calculate the predicted rating using dot product
                predicted_rating = np.dot(user_factors, item_factors)

                # Append both actual and predicted rating since we have them both
                predicted_ratings.append(predicted_rating)
                actual_ratings.append(test_df[test_df['userId'] == user_id][test_df['movieId'] == movie_id]['rating'].values[0])


            except KeyError:
                # If movie not found in user_item_matrix (cold start issue), skip it for rating prediction
                pass

            except IndexError: #Handle case where movie is not found for the user in test_df
                pass

        if len(actual_ratings) > 0 and len(predicted_ratings) > 0: #Check to avoid errors if no ratings were predicted for the user
            rmses.append(np.sqrt(mean_squared_error(actual_ratings, predicted_ratings)))
            maes.append(mean_absolute_error(actual_ratings, predicted_ratings))

        # 2. Recommendation Relevance Evaluation (Precision/Recall)
        # ... (rest of the code remains the same) ...

import streamlit as st
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# Load data
@st.cache_data
def load_data():
    movies = pd.read_csv('movies.csv')
    ratings = pd.read_csv('ratings.csv')
    return movies, ratings

movies_df, ratings_df = load_data()

st.title("ðŸŽ¬ Movie Recommendation System")

# Merge and create tags
movies_df['genres'] = movies_df['genres'].fillna('')
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies_df['genres'])

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
indices = pd.Series(movies_df.index, index=movies_df['title']).drop_duplicates()

def get_recommendations(title, cosine_sim=cosine_sim):
    idx = indices.get(title)
    if idx is None:
        return []
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:6]
    movie_indices = [i[0] for i in sim_scores]
    return movies_df['title'].iloc[movie_indices]

movie_list = movies_df['title'].dropna().unique()
selected_movie = st.selectbox("Choose a movie to get recommendations:", movie_list)

if st.button("Recommend"):
    with st.spinner("Finding recommendations..."):
        recommendations = get_recommendations(selected_movie)
        if recommendations:
            st.subheader("You might also like:")
            for rec in recommendations:
                st.write(f"ðŸŽ¥ {rec}")
        else:
            st.warning("Sorry, no recommendations found.")


